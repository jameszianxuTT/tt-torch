name: Run Model Tests

on:
  workflow_dispatch:
  workflow_call:
    inputs:
      docker-image:
        description: 'Docker image to use for build'
        required: true
        type: string
  workflow_run:
    workflows: [Build]
    types: [completed]

jobs:
  tests:
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        build: [
          {
            runs-on: wormhole_b0, name: "qwen", tests: "
              tests/models/Qwen/test_qwen2_casual_lm.py::test_qwen2_casual_lm
              tests/models/Qwen/test_qwen2_token_classification.py::test_qwen2_token_classification
              tests/models/deepseek/test_deepseek_qwen.py::test_deepseek_qwen
              "
          },
          {
            runs-on: wormhole_b0, name: "albert", tests: "
              tests/models/albert/test_albert_masked_lm.py::test_albert_masked_lm
              tests/models/albert/test_albert_question_answering.py::test_albert_question_answering
              tests/models/albert/test_albert_sequence_classification.py::test_albert_sequence_classification
              tests/models/albert/test_albert_token_classification.py::test_albert_token_classification
              "
          },
          {
            runs-on: wormhole_b0, name: "autoencoder", tests: "
              tests/models/autoencoder_conv/test_autoencoder_conv.py::test_autoencoder_conv
              tests/models/autoencoder_conv/test_autoencoder_conv_v2.py::test_autoencoder_conv_v2
              tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear
              "
          },
          {
            runs-on: wormhole_b0, name: "bert", tests: "
              tests/models/bert/test_bert.py::test_bert
              tests/models/distilbert/test_distilbert.py::test_distilbert
              tests/models/squeeze_bert/test_squeeze_bert.py::test_squeeze_bert
              "
          },
          {
            runs-on: wormhole_b0, name: "RMBG", tests: "
              tests/models/RMBG/test_RMBG.py::test_RMBG
              "
          },
          {
            runs-on: wormhole_b0, name: "resnet", tests: "
              tests/models/resnet/test_resnet.py::test_resnet
              tests/models/resnet50/test_resnet50.py::test_resnet
              "
          },
          {
            runs-on: wormhole_b0, name: "musicgen_small", tests: "
              tests/models/musicgen_small/test_musicgen_small.py::test_musicgen_small
              "
          },
          {
            runs-on: wormhole_b0, name: "whisper", tests: "
              tests/models/whisper/test_whisper.py::test_whisper
              "
          },
          {
            runs-on: wormhole_b0, name: "roberta", tests: "
              tests/models/roberta/test_roberta.py::test_roberta
              "
          },
          {
            runs-on: wormhole_b0, name: "torchvision_1", tests: "
              tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[op_by_op_torch-eval-googlenet]
              tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[op_by_op_torch-eval-densenet201]
              tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[op_by_op_torch-eval-mobilenet_v2]
              "
          },
        ]
    runs-on:
      - ${{ matrix.build.runs-on }}

    name: "tests (${{ matrix.build.runs-on }}, ${{ matrix.build.name }})"

    container:
      image: ${{ inputs.docker-image }}
      options: --user root --device /dev/tenstorrent/0 --shm-size=4gb
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env
        - /mnt/dockercache:/mnt/dockercache

    steps:
    - uses: actions/checkout@v4
      with:
        submodules: recursive
        lfs: true

    - name: Fetch job id
      id: fetch-job-id
      uses: tenstorrent/tt-github-actions/.github/actions/job_id@main
      with:
        job_name: "tests (${{ matrix.build.runs-on }}, ${{ matrix.build.name }})"

    - name: Set reusable strings
      id: strings
      shell: bash
      env:
        JOB_ID: ${{ steps.fetch-job-id.outputs.job_id }}
      run: |
        echo "work-dir=$(pwd)" >> "$GITHUB_OUTPUT"
        echo "install-dir=$(pwd)/install" >> "$GITHUB_OUTPUT"
        echo "dist-dir=$(pwd)/dist" >> "$GITHUB_OUTPUT"
        echo "test-output-dir=$(pwd)/results/models/tests/" >> "$GITHUB_OUTPUT"

    - name: Git safe dir
      run: git config --global --add safe.directory ${{ steps.strings.outputs.work-dir }}

    - name: Use build artifacts
      uses: actions/download-artifact@v4
      with:
        name: install-artifacts
        path: ${{ steps.strings.outputs.install-dir }}

    - name: 'Untar install directory'
      shell: bash
      working-directory: ${{ steps.strings.outputs.install-dir }}
      run: |
        tar xvf artifact.tar
        mkdir -p ${{ steps.strings.outputs.dist-dir }}
        mv wheels/* ${{ steps.strings.outputs.dist-dir }}

    - name: install tt-torch
      shell: bash
      run: |
        source env/activate
        pip install ${{ steps.strings.outputs.dist-dir }}/*.whl

    - name: Run Model Tests
      env:
        HF_HOME: /mnt/dockercache/huggingface
        TORCH_HOME: /mnt/dockercache/torch
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      shell: bash
      run: |
        source env/activate
        apt-get update
        apt install -y libgl1 libglx-mesa0

        # Make sure we don't stop on first failure
        set +e

        tests_list=$(echo "${{ matrix.build.tests }}" | xargs -n1 echo)
        total_tests=$(echo "$tests_list" | wc -l)

        failures=0
        counter=0

        for test_case in $tests_list; do
          counter=$((counter + 1))

          pytest_log="test_${counter}.log"

          pytest -svv "$test_case" --op_by_op_torch > "$pytest_log" 2>&1
          exit_code=$?

          echo "====== BEGIN LOG: $test_case ======" >> full_job_output.log
          cat "$pytest_log" >> full_job_output.log
          echo "====== END LOG: $test_case ========" >> full_job_output.log
          echo >> full_job_output.log
          rm "$pytest_log"

          if [ $exit_code -eq 0 ]; then
            echo "[ $counter / $total_tests ] $test_case PASSED"
          else
            echo "[ $counter / $total_tests ] $test_case FAILED"
            failures=$((failures + 1))
          fi
        done


        # If any test failed, exit nonzero to mark the job as failed
        if [ $failures -ne 0 ]; then
          echo "Total failures: $failures"
          exit 1
        fi

    - name: Tar results
      if: success() || failure()
      shell: bash
      run: |
        TEST_DIR="${{ steps.strings.outputs.test-output-dir }}"
        OUTPUT_TAR="${{ matrix.build.name }}_${{ steps.fetch-job-id.outputs.job_id }}.tar"

        if [ ! -d "$TEST_DIR" ]; then
          echo "ERROR: Test output dir '$TEST_DIR' does not exist. Please check if test ran properly."
          exit 1
        fi

        cd "$TEST_DIR"
        tar cvf "$OUTPUT_TAR" .

    - name: Upload test folder to archive
      if: success() || failure()
      uses: actions/upload-artifact@v4
      with:
        name: test-reports-${{ matrix.build.name }}.tar
        path: ${{ steps.strings.outputs.test-output-dir }}/${{ matrix.build.name }}_${{ steps.fetch-job-id.outputs.job_id }}.tar

    - name: Upload full logs
      if: success() || failure()
      uses: actions/upload-artifact@v4
      with:
        name: full-logs-${{ matrix.build.name }}
        path: full_job_output.log
