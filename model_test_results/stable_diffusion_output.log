============================= test session starts ==============================
platform linux -- Python 3.11.0rc1, pytest-8.3.5, pluggy-1.5.0 -- /localdev/ddilbaz/tt-torch/env/venv/bin/python3.11
cachedir: .pytest_cache
rootdir: /localdev/ddilbaz/tt-torch
collecting ... collected 2 items

tests/models/stable_diffusion/test_stable_diffusion.py::test_stable_diffusion[True-eval] SKIPPED
tests/models/stable_diffusion/test_stable_diffusion_v2.py::test_stable_diffusion_v2[True-eval] FAILED

=================================== FAILURES ===================================
_____________________ test_stable_diffusion_v2[True-eval] ______________________

self = <torch._dynamo.output_graph.OutputGraph object at 0x7ff0dc666750>
gm = GraphModule(
  (L__self___time_embedding_linear_1): Linear(in_features=320, out_features=1280, bias=True)
  (L__self__...20, eps=1e-05, affine=True)
  (L__self___conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

    def _call_user_compiler(self, gm: fx.GraphModule) -> CompiledFn:
        assert self.compiler_fn is not None
        tot = 0
        placeholders = []
        for node in gm.graph.nodes:
            if node.op in ("call_function", "call_method", "call_module"):
                tot += 1
            if node.op == "placeholder":
                placeholders.append(node)
        increment_op_count(tot)
        for pl in placeholders:
            arg = pl.meta["grapharg"]
            # TODO: Why isn't this stored in meta :think:
            pl._dynamo_source = arg.source

        gm._param_name_to_source = self.param_name_to_source  # type: ignore[assignment]
        gm._source_to_user_stacks = self.source_to_user_stacks  # type: ignore[assignment]

        try:
            name = (
                self.compiler_fn.__name__
                if hasattr(self.compiler_fn, "__name__")
                else ""
            )
            _step_logger()(logging.INFO, f"calling compiler function {name}")
            compiler_fn = self.compiler_fn
            if config.verify_correctness:
                compiler_fn = WrapperBackend(compiler_fn)
>           compiled_fn = compiler_fn(gm, self.example_inputs())

env/venv/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1446:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
env/venv/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py:129: in __call__
    compiled_gm = compiler_fn(gm, example_inputs)
env/venv/lib/python3.11/site-packages/torch/__init__.py:2280: in __call__
    return self.compiler_fn(model_, inputs_, **self.kwargs)
tt_torch/dynamo/backend.py:521: in backend
    return aot_module_simplified(
env/venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1071: in aot_module_simplified
    compiled_fn = dispatch_and_compile()
env/venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1056: in dispatch_and_compile
    compiled_fn, _ = create_aot_dispatcher_function(
env/venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:522: in create_aot_dispatcher_function
    return _create_aot_dispatcher_function(
env/venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:759: in _create_aot_dispatcher_function
    compiled_fn, fw_metadata = compiler_fn(
env/venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:179: in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
tt_torch/dynamo/backend.py:472: in _base_backend
    gm, graph_constants = pass_pipeline(gm, example_inputs, compiler_config)
tt_torch/dynamo/passes.py:240: in pass_pipeline
    gm, parameters = inline_parameters(gm)
tt_torch/dynamo/passes.py:176: in inline_parameters
    parameters[node.target] = getattr(gm, node.target).data
env/venv/lib/python3.11/site-packages/torch/utils/_stats.py:21: in wrapper
    return fn(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1238: in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1692: in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1348: in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1790: in _dispatch_impl
    (flat_args, flat_arg_fake_tensors) = self.validate_and_convert_non_fake_tensors(
env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:2094: in validate_and_convert_non_fake_tensors
    validated_args = [validate(a) for a in flat_args]
env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:2094: in <listcomp>
    validated_args = [validate(a) for a in flat_args]
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

x = tensor([0])

    def validate(x: T) -> Union[T, FakeTensor]:
        if not isinstance(x, Tensor):
            return x

        nonlocal flat_arg_fake_tensors
        if not self.is_our_fake(x):
            if torch.Tag.inplace_view in func.tags:
                args, kwargs = pytree.tree_unflatten(flat_args, args_spec)
                raise AssertionError(
                    f"Can't call metadata mutating ops on non-Fake Tensor inputs. Found in {render_call(func, args, kwargs)}"
                )
            if not self.allow_non_fake_inputs:
                if isinstance(x, FakeTensor) and x.fake_mode is not self:
                    raise AssertionError("Mixing fake modes NYI")
                args, kwargs = pytree.tree_unflatten(flat_args, args_spec)
>               raise AssertionError(
                    f"Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode "
                    f"with 'allow_non_fake_inputs'. Found in {render_call(func, args, kwargs)}"
                )
E               AssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.detach.default(tensor([...], size=(1,)))

env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:2082: AssertionError

The above exception was the direct cause of the following exception:

record_property = <function record_property.<locals>.append_property at 0x7ff0dd6b9a80>
mode = 'eval', nightly = True

    @pytest.mark.parametrize(
        "mode",
        ["eval"],
    )
    def test_stable_diffusion_v2(record_property, mode, nightly):
        model_name = "Stable Diffusion V2"
        record_property("model_name", model_name)
        record_property("mode", mode)

        cc = CompilerConfig()
        cc.enable_consteval = True
        cc.consteval_parameters = True
        if nightly:
            cc.compile_depth = CompileDepth.EXECUTE_OP_BY_OP

        tester = ThisTester(model_name, mode, compiler_config=cc)
>       results = tester.test_model()

tests/models/stable_diffusion/test_stable_diffusion_v2.py:79:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/utils.py:165: in test_model
    return self.test_model_eval(on_device)
env/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
tests/utils.py:157: in test_model_eval
    outputs = self.run_model(model, inputs)
tests/utils.py:81: in run_model
    return model(**inputs)
env/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:465: in _fn
    return fn(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1269: in __call__
    return self._torchdynamo_orig_callable(
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1064: in __call__
    result = self._inner_convert(
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:526: in __call__
    return _compile(
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:924: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:666: in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
env/venv/lib/python3.11/site-packages/torch/_utils_internal.py:87: in wrapper_function
    return function(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:699: in _compile_inner
    out_code = transform_code_object(code, transform)
env/venv/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py:1322: in transform_code_object
    transformations(instructions, code_options)
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:219: in _fn
    return fn(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:634: in transform
    tracer.run()
env/venv/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2796: in run
    super().run()
env/venv/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:983: in run
    while self.step():
env/venv/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:895: in step
    self.dispatch_table[inst.opcode](self, inst)
env/venv/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2987: in RETURN_VALUE
    self._return(inst)
env/venv/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2972: in _return
    self.output.compile_subgraph(
env/venv/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1142: in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
env/venv/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1369: in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
env/venv/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1416: in call_user_compiler
    return self._call_user_compiler(gm)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <torch._dynamo.output_graph.OutputGraph object at 0x7ff0dc666750>
gm = GraphModule(
  (L__self___time_embedding_linear_1): Linear(in_features=320, out_features=1280, bias=True)
  (L__self__...20, eps=1e-05, affine=True)
  (L__self___conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

    def _call_user_compiler(self, gm: fx.GraphModule) -> CompiledFn:
        assert self.compiler_fn is not None
        tot = 0
        placeholders = []
        for node in gm.graph.nodes:
            if node.op in ("call_function", "call_method", "call_module"):
                tot += 1
            if node.op == "placeholder":
                placeholders.append(node)
        increment_op_count(tot)
        for pl in placeholders:
            arg = pl.meta["grapharg"]
            # TODO: Why isn't this stored in meta :think:
            pl._dynamo_source = arg.source

        gm._param_name_to_source = self.param_name_to_source  # type: ignore[assignment]
        gm._source_to_user_stacks = self.source_to_user_stacks  # type: ignore[assignment]

        try:
            name = (
                self.compiler_fn.__name__
                if hasattr(self.compiler_fn, "__name__")
                else ""
            )
            _step_logger()(logging.INFO, f"calling compiler function {name}")
            compiler_fn = self.compiler_fn
            if config.verify_correctness:
                compiler_fn = WrapperBackend(compiler_fn)
            compiled_fn = compiler_fn(gm, self.example_inputs())
            _step_logger()(logging.INFO, f"done compiler function {name}")
            assert callable(compiled_fn), "compiler_fn did not return callable"
        except exceptions_allowed_to_be_fallback as e:
            if self.has_user_defined_allowed_in_graph:
                raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
                    e.__traceback__
                ) from None
            msg = (
                "Backend compiler failed with a fake tensor exception at \n"
                f"{self.root_tx.format_frame_summary()}"
                "Adding a graph break."
            )
            unimplemented_with_warning(e, self.root_tx.f_code, msg)
        except SkipFrame as e:
            # The backend compiler has requested that we skip the frame, instead of
            # aborting execution.
            raise e
        except Exception as e:
>           raise BackendCompilerFailed(self.compiler_fn, e) from e
E           torch._dynamo.exc.BackendCompilerFailed: backend='backend' raised:
E           AssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.detach.default(tensor([...], size=(1,)))
E
E           Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E
E
E           You can suppress this exception and fall back to eager by setting:
E               import torch._dynamo
E               torch._dynamo.config.suppress_errors = True

env/venv/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1465: BackendCompilerFailed
=============================== warnings summary ===============================
tests/models/stable_diffusion/test_stable_diffusion_v2.py::test_stable_diffusion_v2[True-eval]
  /localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/diffusers/schedulers/scheduling_lms_discrete.py:169: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.
    sigmas = np.array(((1 - self.alphas_cumprod) / self.alphas_cumprod) ** 0.5)

tests/models/stable_diffusion/test_stable_diffusion_v2.py::test_stable_diffusion_v2[True-eval]
tests/models/stable_diffusion/test_stable_diffusion_v2.py::test_stable_diffusion_v2[True-eval]
  /localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/diffusers/schedulers/scheduling_lms_discrete.py:297: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.
    sigmas = np.array(((1 - self.alphas_cumprod) / self.alphas_cumprod) ** 0.5)

tests/models/stable_diffusion/test_stable_diffusion_v2.py::test_stable_diffusion_v2[True-eval]
  /localdev/ddilbaz/tt-torch/tests/models/stable_diffusion/test_stable_diffusion_v2.py:43: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.
    (batch_size, self.model.in_channels, height // 8, width // 8)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/models/stable_diffusion/test_stable_diffusion_v2.py::test_stable_diffusion_v2[True-eval] - torch._dynamo.exc.BackendCompilerFailed: backend='backend' raised:
AssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.detach.default(tensor([...], size=(1,)))

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True
============= 1 failed, 1 skipped, 4 warnings in 128.25s (0:02:08) =============
