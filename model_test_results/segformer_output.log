============================= test session starts ==============================
platform linux -- Python 3.11.0rc1, pytest-8.3.5, pluggy-1.5.0 -- /localdev/ddilbaz/tt-torch/env/venv/bin/python3.11
cachedir: .pytest_cache
rootdir: /localdev/ddilbaz/tt-torch
collecting ... collected 2 items

tests/models/segformer/test_segformer.py::test_segformer[True-train] SKIPPED
tests/models/segformer/test_segformer.py::test_segformer[True-eval] Traceback (most recent call last):
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/passes/shape_prop.py", line 152, in run_node
    result = super().run_node(n)
             ^^^^^^^^^^^^^^^^^^^
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/interpreter.py", line 203, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/interpreter.py", line 275, in call_function
    return target(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/torch/_ops.py", line 716, in __call__
    return self._op(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/torch/utils/_stats.py", line 21, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1238, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1692, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1339, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1983, in _dispatch_impl
    op_impl_out = op_impl(self, func, *args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py", line 147, in dispatch_to_op_implementations_dict
    return op_implementations_dict[func](fake_mode, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py", line 670, in conv
    batch = kwargs["input"].shape[0]
            ~~~~~~~~~~~~~~~~~~~~~^^^
IndexError: tuple index out of range
FAILED

=================================== FAILURES ===================================
__________________________ test_segformer[True-eval] ___________________________

self = <torch.fx.passes.shape_prop.ShapeProp object at 0x7f5180138cd0>
n = convolution

    def run_node(self, n : Node) -> Any:
        try:
            if self.fake_module is not None:
                # Hacky swap. Alternatively, we could do this with overriding
                # call_module and get_attr.
                self.module = self.fake_module
            try:
                if self.fake_mode is not None:
                    with self.fake_mode, enable_python_dispatcher():
>                       result = super().run_node(n)

env/venv/lib/python3.11/site-packages/torch/fx/passes/shape_prop.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
env/venv/lib/python3.11/site-packages/torch/fx/interpreter.py:203: in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
env/venv/lib/python3.11/site-packages/torch/fx/interpreter.py:275: in call_function
    return target(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/_ops.py:716: in __call__
    return self._op(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/utils/_stats.py:21: in wrapper
    return fn(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1238: in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1692: in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1339: in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1983: in _dispatch_impl
    op_impl_out = op_impl(self, func, *args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py:147: in dispatch_to_op_implementations_dict
    return op_implementations_dict[func](fake_mode, func, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

fake_mode = <torch._subclasses.fake_tensor.FakeTensorMode object at 0x7f51800a1850>
func = <OpOverload(op='aten.convolution', overload='default')>
args = (FakeTensor(..., size=(), dtype=torch.int64), FakeTensor(..., size=(32, 3, 7, 7), dtype=torch.bfloat16), FakeTensor(..., size=(32,), dtype=torch.bfloat16), [4, 4], [3, 3], [1, 1], ...)
kwargs = {'bias': FakeTensor(..., size=(32,), dtype=torch.bfloat16), 'dilation': [1, 1], 'groups': 1, 'input': FakeTensor(..., size=(), dtype=torch.int64), ...}
_ = (), k = 4

    @register_op_impl([aten.convolution.default, aten.convolution_backward.default])
    def conv(fake_mode, func, *args, **kwargs):
        _, kwargs = normalize_function(
            func, args=args, kwargs=kwargs, normalize_to_only_use_kwargs=True
        )
        device = kwargs["input"].fake_device
        # need to re-enable mode so the tensors report fake device
        with fake_mode:
            # if the input is unsqueezed is done in Convolution.cpp we get segfault
            k = kwargs["weight"].ndim
>           batch = kwargs["input"].shape[0]
E           IndexError: tuple index out of range

env/venv/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py:670: IndexError

The above exception was the direct cause of the following exception:

self = <torch._dynamo.output_graph.OutputGraph object at 0x7f5181339fd0>
gm = GraphModule(
  (L__self___segformer_encoder_patch_embeddings_0_proj): Conv2d(3, 32, kernel_size=(7, 7), stride=(4, 4),...opout(p=0.1, inplace=False)
  (L__self___decode_head_classifier): Conv2d(256, 150, kernel_size=(1, 1), stride=(1, 1))
)

    def _call_user_compiler(self, gm: fx.GraphModule) -> CompiledFn:
        assert self.compiler_fn is not None
        tot = 0
        placeholders = []
        for node in gm.graph.nodes:
            if node.op in ("call_function", "call_method", "call_module"):
                tot += 1
            if node.op == "placeholder":
                placeholders.append(node)
        increment_op_count(tot)
        for pl in placeholders:
            arg = pl.meta["grapharg"]
            # TODO: Why isn't this stored in meta :think:
            pl._dynamo_source = arg.source

        gm._param_name_to_source = self.param_name_to_source  # type: ignore[assignment]
        gm._source_to_user_stacks = self.source_to_user_stacks  # type: ignore[assignment]

        try:
            name = (
                self.compiler_fn.__name__
                if hasattr(self.compiler_fn, "__name__")
                else ""
            )
            _step_logger()(logging.INFO, f"calling compiler function {name}")
            compiler_fn = self.compiler_fn
            if config.verify_correctness:
                compiler_fn = WrapperBackend(compiler_fn)
>           compiled_fn = compiler_fn(gm, self.example_inputs())

env/venv/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1446:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
env/venv/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py:129: in __call__
    compiled_gm = compiler_fn(gm, example_inputs)
env/venv/lib/python3.11/site-packages/torch/__init__.py:2280: in __call__
    return self.compiler_fn(model_, inputs_, **self.kwargs)
tt_torch/dynamo/backend.py:521: in backend
    return aot_module_simplified(
env/venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1071: in aot_module_simplified
    compiled_fn = dispatch_and_compile()
env/venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1056: in dispatch_and_compile
    compiled_fn, _ = create_aot_dispatcher_function(
env/venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:522: in create_aot_dispatcher_function
    return _create_aot_dispatcher_function(
env/venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:759: in _create_aot_dispatcher_function
    compiled_fn, fw_metadata = compiler_fn(
env/venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:179: in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
tt_torch/dynamo/backend.py:472: in _base_backend
    gm, graph_constants = pass_pipeline(gm, example_inputs, compiler_config)
tt_torch/dynamo/passes.py:257: in pass_pipeline
    run_shape_prop(gm, example_inputs + constant_inputs)
tt_torch/dynamo/passes.py:31: in run_shape_prop
    shape_prop.run(*fake_args)
env/venv/lib/python3.11/site-packages/torch/fx/interpreter.py:146: in run
    self.env[node] = self.run_node(node)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <torch.fx.passes.shape_prop.ShapeProp object at 0x7f5180138cd0>
n = convolution

    def run_node(self, n : Node) -> Any:
        try:
            if self.fake_module is not None:
                # Hacky swap. Alternatively, we could do this with overriding
                # call_module and get_attr.
                self.module = self.fake_module
            try:
                if self.fake_mode is not None:
                    with self.fake_mode, enable_python_dispatcher():
                        result = super().run_node(n)
                else:
                    result = super().run_node(n)
            finally:
                self.module = self.real_module
        except Exception as e:
            traceback.print_exc()
>           raise RuntimeError(
                f"ShapeProp error for: node={n.format_node()} with "
                f"meta={n.meta}"
            ) from e
E           RuntimeError: ShapeProp error for: node=%convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg208_1, %arg0_1, %arg1_1, [4, 4], [3, 3], [1, 1], False, [0, 0], 1), kwargs = {}) with meta={'stack_trace': '  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 790, in forward\n    outputs = self.segformer(\n  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 544, in forward\n    encoder_outputs = self.encoder(\n  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 418, in forward\n    hidden_states, height, width = embedding_layer(hidden_states)\n  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 133, in forward\n    embeddings = self.proj(pixel_values)\n', 'nn_module_stack': {'L__self___segformer': ("L['self'].segformer", <class 'transformers.models.segformer.modeling_segformer.SegformerModel'>), 'L__self___segformer_encoder': ("L['self'].segformer.encoder", <class 'transformers.models.segformer.modeling_segformer.SegformerEncoder'>), 'L__self___segformer_encoder_patch_embeddings_0': ("L['self'].segformer.encoder.patch_embeddings[0]", <class 'transformers.models.segformer.modeling_segformer.SegformerOverlapPatchEmbeddings'>), 'L__self___segformer_encoder_patch_embeddings_0_proj': ("L['self'].segformer.encoder.patch_embeddings[0].proj", <class 'torch.nn.modules.conv.Conv2d'>)}, 'source_fn_stack': [('l__self___segformer_encoder_patch_embeddings_0_proj', <class 'torch.nn.modules.conv.Conv2d'>)], 'original_aten': <OpOverload(op='aten.convolution', overload='default')>, 'from_node': [('embeddings', 'L__self___segformer_encoder_patch_embeddings_0_proj')], 'seq_nr': 819, 'val': FakeTensor(..., size=(1, 32, 128, 128), dtype=torch.bfloat16), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 32, 128, 128]), dtype=torch.bfloat16, requires_grad=False, stride=(524288, 16384, 128, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})}
E
E           While executing %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg208_1, %arg0_1, %arg1_1, [4, 4], [3, 3], [1, 1], False, [0, 0], 1), kwargs = {})
E           Original traceback:
E             File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 790, in forward
E               outputs = self.segformer(
E             File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 544, in forward
E               encoder_outputs = self.encoder(
E             File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 418, in forward
E               hidden_states, height, width = embedding_layer(hidden_states)
E             File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 133, in forward
E               embeddings = self.proj(pixel_values)

env/venv/lib/python3.11/site-packages/torch/fx/passes/shape_prop.py:159: RuntimeError

The above exception was the direct cause of the following exception:

record_property = <function record_property.<locals>.append_property at 0x7f518137bce0>
mode = 'eval', nightly = True

    @pytest.mark.parametrize(
        "mode",
        ["train", "eval"],
    )
    def test_segformer(record_property, mode, nightly):
        if mode == "train":
            pytest.skip()
        model_name = "SegFormer"
        record_property("model_name", model_name)
        record_property("mode", mode)

        cc = CompilerConfig()
        cc.enable_consteval = True
        cc.consteval_parameters = True
        if nightly:
            cc.compile_depth = CompileDepth.EXECUTE_OP_BY_OP
        else:
            cc.compile_depth = CompileDepth.TTNN_IR

        tester = ThisTester(model_name, mode, compiler_config=cc)
>       results = tester.test_model()

tests/models/segformer/test_segformer.py:64:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/utils.py:165: in test_model
    return self.test_model_eval(on_device)
env/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
tests/utils.py:157: in test_model_eval
    outputs = self.run_model(model, inputs)
tests/utils.py:81: in run_model
    return model(**inputs)
env/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:465: in _fn
    return fn(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1269: in __call__
    return self._torchdynamo_orig_callable(
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1064: in __call__
    result = self._inner_convert(
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:526: in __call__
    return _compile(
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:924: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:666: in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
env/venv/lib/python3.11/site-packages/torch/_utils_internal.py:87: in wrapper_function
    return function(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:699: in _compile_inner
    out_code = transform_code_object(code, transform)
env/venv/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py:1322: in transform_code_object
    transformations(instructions, code_options)
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:219: in _fn
    return fn(*args, **kwargs)
env/venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:634: in transform
    tracer.run()
env/venv/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2796: in run
    super().run()
env/venv/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:983: in run
    while self.step():
env/venv/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:895: in step
    self.dispatch_table[inst.opcode](self, inst)
env/venv/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2987: in RETURN_VALUE
    self._return(inst)
env/venv/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2972: in _return
    self.output.compile_subgraph(
env/venv/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1142: in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
env/venv/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1369: in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
env/venv/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1416: in call_user_compiler
    return self._call_user_compiler(gm)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <torch._dynamo.output_graph.OutputGraph object at 0x7f5181339fd0>
gm = GraphModule(
  (L__self___segformer_encoder_patch_embeddings_0_proj): Conv2d(3, 32, kernel_size=(7, 7), stride=(4, 4),...opout(p=0.1, inplace=False)
  (L__self___decode_head_classifier): Conv2d(256, 150, kernel_size=(1, 1), stride=(1, 1))
)

    def _call_user_compiler(self, gm: fx.GraphModule) -> CompiledFn:
        assert self.compiler_fn is not None
        tot = 0
        placeholders = []
        for node in gm.graph.nodes:
            if node.op in ("call_function", "call_method", "call_module"):
                tot += 1
            if node.op == "placeholder":
                placeholders.append(node)
        increment_op_count(tot)
        for pl in placeholders:
            arg = pl.meta["grapharg"]
            # TODO: Why isn't this stored in meta :think:
            pl._dynamo_source = arg.source

        gm._param_name_to_source = self.param_name_to_source  # type: ignore[assignment]
        gm._source_to_user_stacks = self.source_to_user_stacks  # type: ignore[assignment]

        try:
            name = (
                self.compiler_fn.__name__
                if hasattr(self.compiler_fn, "__name__")
                else ""
            )
            _step_logger()(logging.INFO, f"calling compiler function {name}")
            compiler_fn = self.compiler_fn
            if config.verify_correctness:
                compiler_fn = WrapperBackend(compiler_fn)
            compiled_fn = compiler_fn(gm, self.example_inputs())
            _step_logger()(logging.INFO, f"done compiler function {name}")
            assert callable(compiled_fn), "compiler_fn did not return callable"
        except exceptions_allowed_to_be_fallback as e:
            if self.has_user_defined_allowed_in_graph:
                raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
                    e.__traceback__
                ) from None
            msg = (
                "Backend compiler failed with a fake tensor exception at \n"
                f"{self.root_tx.format_frame_summary()}"
                "Adding a graph break."
            )
            unimplemented_with_warning(e, self.root_tx.f_code, msg)
        except SkipFrame as e:
            # The backend compiler has requested that we skip the frame, instead of
            # aborting execution.
            raise e
        except Exception as e:
>           raise BackendCompilerFailed(self.compiler_fn, e) from e
E           torch._dynamo.exc.BackendCompilerFailed: backend='backend' raised:
E           RuntimeError: ShapeProp error for: node=%convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg208_1, %arg0_1, %arg1_1, [4, 4], [3, 3], [1, 1], False, [0, 0], 1), kwargs = {}) with meta={'stack_trace': '  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 790, in forward\n    outputs = self.segformer(\n  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 544, in forward\n    encoder_outputs = self.encoder(\n  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 418, in forward\n    hidden_states, height, width = embedding_layer(hidden_states)\n  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 133, in forward\n    embeddings = self.proj(pixel_values)\n', 'nn_module_stack': {'L__self___segformer': ("L['self'].segformer", <class 'transformers.models.segformer.modeling_segformer.SegformerModel'>), 'L__self___segformer_encoder': ("L['self'].segformer.encoder", <class 'transformers.models.segformer.modeling_segformer.SegformerEncoder'>), 'L__self___segformer_encoder_patch_embeddings_0': ("L['self'].segformer.encoder.patch_embeddings[0]", <class 'transformers.models.segformer.modeling_segformer.SegformerOverlapPatchEmbeddings'>), 'L__self___segformer_encoder_patch_embeddings_0_proj': ("L['self'].segformer.encoder.patch_embeddings[0].proj", <class 'torch.nn.modules.conv.Conv2d'>)}, 'source_fn_stack': [('l__self___segformer_encoder_patch_embeddings_0_proj', <class 'torch.nn.modules.conv.Conv2d'>)], 'original_aten': <OpOverload(op='aten.convolution', overload='default')>, 'from_node': [('embeddings', 'L__self___segformer_encoder_patch_embeddings_0_proj')], 'seq_nr': 819, 'val': FakeTensor(..., size=(1, 32, 128, 128), dtype=torch.bfloat16), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 32, 128, 128]), dtype=torch.bfloat16, requires_grad=False, stride=(524288, 16384, 128, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})}
E
E           While executing %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg208_1, %arg0_1, %arg1_1, [4, 4], [3, 3], [1, 1], False, [0, 0], 1), kwargs = {})
E           Original traceback:
E             File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 790, in forward
E               outputs = self.segformer(
E             File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 544, in forward
E               encoder_outputs = self.encoder(
E             File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 418, in forward
E               hidden_states, height, width = embedding_layer(hidden_states)
E             File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 133, in forward
E               embeddings = self.proj(pixel_values)
E
E
E           Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E
E
E           You can suppress this exception and fall back to eager by setting:
E               import torch._dynamo
E               torch._dynamo.config.suppress_errors = True

env/venv/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1465: BackendCompilerFailed
=============================== warnings summary ===============================
tests/models/segformer/test_segformer.py::test_segformer[True-eval]
  /localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'
    return func(*args, **kwargs)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/models/segformer/test_segformer.py::test_segformer[True-eval] - torch._dynamo.exc.BackendCompilerFailed: backend='backend' raised:
RuntimeError: ShapeProp error for: node=%convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg208_1, %arg0_1, %arg1_1, [4, 4], [3, 3], [1, 1], False, [0, 0], 1), kwargs = {}) with meta={'stack_trace': '  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 790, in forward\n    outputs = self.segformer(\n  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 544, in forward\n    encoder_outputs = self.encoder(\n  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 418, in forward\n    hidden_states, height, width = embedding_layer(hidden_states)\n  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 133, in forward\n    embeddings = self.proj(pixel_values)\n', 'nn_module_stack': {'L__self___segformer': ("L['self'].segformer", <class 'transformers.models.segformer.modeling_segformer.SegformerModel'>), 'L__self___segformer_encoder': ("L['self'].segformer.encoder", <class 'transformers.models.segformer.modeling_segformer.SegformerEncoder'>), 'L__self___segformer_encoder_patch_embeddings_0': ("L['self'].segformer.encoder.patch_embeddings[0]", <class 'transformers.models.segformer.modeling_segformer.SegformerOverlapPatchEmbeddings'>), 'L__self___segformer_encoder_patch_embeddings_0_proj': ("L['self'].segformer.encoder.patch_embeddings[0].proj", <class 'torch.nn.modules.conv.Conv2d'>)}, 'source_fn_stack': [('l__self___segformer_encoder_patch_embeddings_0_proj', <class 'torch.nn.modules.conv.Conv2d'>)], 'original_aten': <OpOverload(op='aten.convolution', overload='default')>, 'from_node': [('embeddings', 'L__self___segformer_encoder_patch_embeddings_0_proj')], 'seq_nr': 819, 'val': FakeTensor(..., size=(1, 32, 128, 128), dtype=torch.bfloat16), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 32, 128, 128]), dtype=torch.bfloat16, requires_grad=False, stride=(524288, 16384, 128, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})}

While executing %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg208_1, %arg0_1, %arg1_1, [4, 4], [3, 3], [1, 1], False, [0, 0], 1), kwargs = {})
Original traceback:
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 790, in forward
    outputs = self.segformer(
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 544, in forward
    encoder_outputs = self.encoder(
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 418, in forward
    hidden_states, height, width = embedding_layer(hidden_states)
  File "/localdev/ddilbaz/tt-torch/env/venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_segformer.py", line 133, in forward
    embeddings = self.proj(pixel_values)


Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True
=================== 1 failed, 1 skipped, 1 warning in 7.86s ====================
