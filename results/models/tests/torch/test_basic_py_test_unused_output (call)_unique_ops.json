{"aten::var_mean.correction_256x256": {"torch_name": "aten::var_mean.correction", "input_shapes": ["torch.Size([256, 256])"], "output_shapes": ["[]", "[]"], "num_ops": 1, "compilation_status": 6, "parsed_stable_hlo_ops": false, "torch_ir_graph": "module {\n  func.func @main(%arg0: !torch.vtensor<[256,256],f32>) -> !torch.vtensor<[],f32> {\n    %none = torch.constant.none\n    %none_0 = torch.constant.none\n    %false = torch.constant.bool false\n    %result0, %result1 = torch.aten.var_mean.correction %arg0, %none, %none_0, %false : !torch.vtensor<[256,256],f32>, !torch.none, !torch.none, !torch.bool -> !torch.vtensor<[],f32>, !torch.vtensor<[],f32>\n    return %result1 : !torch.vtensor<[],f32>\n  }\n}\n", "stable_hlo_graph": "module {\n  func.func @main(%arg0: tensor<256x256xf32>) -> tensor<f32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %cst_0 = arith.constant dense<65536> : tensor<1xi64>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0, 1] : (tensor<256x256xf32>, tensor<f32>) -> tensor<f32>\n    %1 = stablehlo.convert %cst_0 : (tensor<1xi64>) -> tensor<1xf32>\n    %2 = stablehlo.reshape %1 : (tensor<1xf32>) -> tensor<f32>\n    %3 = stablehlo.divide %0, %2 : tensor<f32>\n    return %3 : tensor<f32>\n  }\n}\n", "stable_hlo_ops": [], "ttir_graph": "module {\n  func.func @main(%arg0: tensor<256x256xf32>) -> tensor<1xf32> {\n    %0 = \"ttir.constant\"() <{value = dense<65536> : tensor<1xi32>}> : () -> tensor<1xi32>\n    %1 = tensor.empty() : tensor<1xf32>\n    %2 = \"ttir.sum\"(%arg0, %1) <{dim_arg = [0 : i32, 1 : i32], keep_dim = false}> : (tensor<256x256xf32>, tensor<1xf32>) -> tensor<1xf32>\n    %3 = tensor.empty() : tensor<1xf32>\n    %4 = \"ttir.typecast\"(%0, %3) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xi32>, tensor<1xf32>) -> tensor<1xf32>\n    %5 = tensor.empty() : tensor<1xf32>\n    %6 = \"ttir.reshape\"(%4, %5) <{shape = [1 : i32]}> : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>\n    %7 = tensor.empty() : tensor<1xf32>\n    %8 = \"ttir.div\"(%2, %6, %7) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>\n    return %8 : tensor<1xf32>\n  }\n}\n", "ttnn_graph": "#device = #tt.device<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>\n#dram = #ttnn.buffer_type<dram>\n#system_desc = #tt.system_desc<[{role = host, target_triple = \"x86_64-pc-linux-gnu\"}], [{arch = <wormhole_b0>, grid = 8x8, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 1024, erisc_l1_unreserved_base = 1024, dram_unreserved_base = 1024, dram_unreserved_end = 1073741824, physical_cores = {worker = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  1x0,  1x1,  1x2,  1x3,  1x4,  1x5,  1x6,  1x7,  2x0,  2x1,  2x2,  2x3,  2x4,  2x5,  2x6,  2x7,  3x0,  3x1,  3x2,  3x3,  3x4,  3x5,  3x6,  3x7,  4x0,  4x1,  4x2,  4x3,  4x4,  4x5,  4x6,  4x7,  5x0,  5x1,  5x2,  5x3,  5x4,  5x5,  5x6,  5x7,  6x0,  6x1,  6x2,  6x3,  6x4,  6x5,  6x6,  6x7,  7x0,  7x1,  7x2,  7x3,  7x4,  7x5,  7x6,  7x7] dram = [ 8x0,  9x0,  10x0,  8x1,  9x1,  10x1,  8x2,  9x2,  10x2,  8x3,  9x3,  10x3]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32}], [0], [3 : i32], [ 0x0x0x0]>\n#system_memory = #ttnn.buffer_type<system_memory>\n#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x256xf32, #system_memory>>\n#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xf32, #system_memory>>\n#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>\n#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x8x!tt.tile<32x32, f32>, #dram>, <interleaved>>\n#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>\n#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>\nmodule attributes {tt.device = #device, tt.system_desc = #system_desc} {\n  func.func @main(%arg0: tensor<256x256xf32, #ttnn_layout>) -> tensor<1xf32, #ttnn_layout1> {\n    %0 = \"ttnn.get_device\"() <{mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !tt.device<#device>\n    %1 = \"ttnn.full\"(%0) <{fillValue = 6.553600e+04 : f32}> : (!tt.device<#device>) -> tensor<1xi32, #ttnn_layout2>\n    %2 = \"ttnn.to_layout\"(%arg0) <{layout = #ttnn.layout<tile>}> : (tensor<256x256xf32, #ttnn_layout>) -> tensor<256x256xf32, #ttnn_layout3>\n    %3 = \"ttnn.to_device\"(%2, %0) <{memory_config = #ttnn.memory_config<#dram, <<8x8>>, <interleaved>>}> : (tensor<256x256xf32, #ttnn_layout3>, !tt.device<#device>) -> tensor<256x256xf32, #ttnn_layout3>\n    \"ttnn.deallocate\"(%2) <{force = false}> : (tensor<256x256xf32, #ttnn_layout3>) -> ()\n    %4 = \"ttnn.sum\"(%3) <{keep_dim = true}> : (tensor<256x256xf32, #ttnn_layout3>) -> tensor<1x1xf32, #ttnn_layout4>\n    \"ttnn.deallocate\"(%3) <{force = false}> : (tensor<256x256xf32, #ttnn_layout3>) -> ()\n    %5 = \"ttnn.reshape\"(%4) <{shape = [1 : i32]}> : (tensor<1x1xf32, #ttnn_layout4>) -> tensor<1xf32, #ttnn_layout5>\n    \"ttnn.deallocate\"(%4) <{force = false}> : (tensor<1x1xf32, #ttnn_layout4>) -> ()\n    %6 = \"ttnn.typecast\"(%1) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1xi32, #ttnn_layout2>) -> tensor<1xf32, #ttnn_layout5>\n    \"ttnn.deallocate\"(%1) <{force = false}> : (tensor<1xi32, #ttnn_layout2>) -> ()\n    %7 = \"ttnn.empty\"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1>}> : (!tt.device<#device>) -> tensor<1xf32, #ttnn_layout5>\n    %8 = \"ttnn.div\"(%5, %6, %7) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1xf32, #ttnn_layout5>, tensor<1xf32, #ttnn_layout5>, tensor<1xf32, #ttnn_layout5>) -> tensor<1xf32, #ttnn_layout5>\n    \"ttnn.deallocate\"(%6) <{force = false}> : (tensor<1xf32, #ttnn_layout5>) -> ()\n    \"ttnn.deallocate\"(%5) <{force = false}> : (tensor<1xf32, #ttnn_layout5>) -> ()\n    %9 = \"ttnn.from_device\"(%8) : (tensor<1xf32, #ttnn_layout5>) -> tensor<1xf32, #ttnn_layout1>\n    \"ttnn.deallocate\"(%7) <{force = false}> : (tensor<1xf32, #ttnn_layout5>) -> ()\n    %10 = \"ttnn.to_layout\"(%9) <{layout = #ttnn.layout<row_major>}> : (tensor<1xf32, #ttnn_layout1>) -> tensor<1xf32, #ttnn_layout1>\n    \"ttnn.deallocate\"(%9) <{force = false}> : (tensor<1xf32, #ttnn_layout1>) -> ()\n    return %10 : tensor<1xf32, #ttnn_layout1>\n  }\n}\n", "runtime_stack_dump": "", "pcc": "", "atol": "", "compiled_json": "{\n  \"version\": {\n    \"major\": 0,\n    \"minor\": 0,\n    \"patch\": 619\n  },\n  \"ttmlir_git_hash\": \"ce1294db83fd285783ab1e4767fbfb09285a36b4\",\n  \"system_desc\": {\n    \"cpu_descs\": [\n      {\n        \"role\": \"Host\",\n        \"target_triple\": \"x86_64-pc-linux-gnu\"\n      }\n    ],\n    \"chip_descs\": [\n      {\n        \"arch\": \"Wormhole_b0\",\n        \"grid_size\": {\n          \"y\": 8,\n          \"x\": 8\n        },\n        \"l1_size\": 1499136,\n        \"num_dram_channels\": 12,\n        \"dram_channel_size\": 1073741824,\n        \"noc_l1_address_align_bytes\": 16,\n        \"pcie_address_align_bytes\": 32,\n        \"noc_dram_address_align_bytes\": 32,\n        \"l1_unreserved_base\": 1024,\n        \"erisc_l1_unreserved_base\": 1024,\n        \"dram_unreserved_base\": 1024,\n        \"dram_unreserved_end\": 1073741824,\n        \"physical_cores\": {\n          \"worker\": [\n            {\n              \"y\": 0,\n              \"x\": 0\n            },\n            {\n              \"y\": 0,\n              \"x\": 1\n            },\n            {\n              \"y\": 0,\n              \"x\": 2\n            },\n            {\n              \"y\": 0,\n              \"x\": 3\n            },\n            {\n              \"y\": 0,\n              \"x\": 4\n            },\n            {\n              \"y\": 0,\n              \"x\": 5\n            },\n            {\n              \"y\": 0,\n              \"x\": 6\n            },\n            {\n              \"y\": 0,\n              \"x\": 7\n            },\n            {\n              \"y\": 1,\n              \"x\": 0\n            },\n            {\n              \"y\": 1,\n              \"x\": 1\n            },\n            {\n              \"y\": 1,\n              \"x\": 2\n            },\n            {\n              \"y\": 1,\n              \"x\": 3\n            },\n            {\n              \"y\": 1,\n              \"x\": 4\n            },\n            {\n              \"y\": 1,\n              \"x\": 5\n            },\n            {\n              \"y\": 1,\n              \"x\": 6\n            },\n            {\n              \"y\": 1,\n              \"x\": 7\n            },\n            {\n              \"y\": 2,\n              \"x\": 0\n            },\n            {\n              \"y\": 2,\n              \"x\": 1\n            },\n            {\n              \"y\": 2,\n              \"x\": 2\n            },\n            {\n              \"y\": 2,\n              \"x\": 3\n            },\n            {\n              \"y\": 2,\n              \"x\": 4\n            },\n            {\n              \"y\": 2,\n              \"x\": 5\n            },\n            {\n              \"y\": 2,\n              \"x\": 6\n            },\n            {\n              \"y\": 2,\n              \"x\": 7\n            },\n            {\n              \"y\": 3,\n              \"x\": 0\n            },\n            {\n              \"y\": 3,\n              \"x\": 1\n            },\n            {\n              \"y\": 3,\n              \"x\": 2\n            },\n            {\n              \"y\": 3,\n              \"x\": 3\n            },\n            {\n              \"y\": 3,\n              \"x\": 4\n            },\n            {\n              \"y\": 3,\n              \"x\": 5\n            },\n            {\n              \"y\": 3,\n              \"x\": 6\n            },\n            {\n              \"y\": 3,\n              \"x\": 7\n            },\n            {\n              \"y\": 4,\n              \"x\": 0\n            },\n            {\n              \"y\": 4,\n              \"x\": 1\n            },\n            {\n              \"y\": 4,\n              \"x\": 2\n            },\n            {\n              \"y\": 4,\n              \"x\": 3\n            },\n            {\n              \"y\": 4,\n              \"x\": 4\n            },\n            {\n              \"y\": 4,\n              \"x\": 5\n            },\n            {\n              \"y\": 4,\n              \"x\": 6\n            },\n            {\n              \"y\": 4,\n              \"x\": 7\n            },\n            {\n              \"y\": 5,\n              \"x\": 0\n            },\n            {\n              \"y\": 5,\n              \"x\": 1\n            },\n            {\n              \"y\": 5,\n              \"x\": 2\n            },\n            {\n              \"y\": 5,\n              \"x\": 3\n            },\n            {\n              \"y\": 5,\n              \"x\": 4\n            },\n            {\n              \"y\": 5,\n              \"x\": 5\n            },\n            {\n              \"y\": 5,\n              \"x\": 6\n            },\n            {\n              \"y\": 5,\n              \"x\": 7\n            },\n            {\n              \"y\": 6,\n              \"x\": 0\n            },\n            {\n              \"y\": 6,\n              \"x\": 1\n            },\n            {\n              \"y\": 6,\n              \"x\": 2\n            },\n            {\n              \"y\": 6,\n              \"x\": 3\n            },\n            {\n              \"y\": 6,\n              \"x\": 4\n            },\n            {\n              \"y\": 6,\n              \"x\": 5\n            },\n            {\n              \"y\": 6,\n              \"x\": 6\n            },\n            {\n              \"y\": 6,\n              \"x\": 7\n            },\n            {\n              \"y\": 7,\n              \"x\": 0\n            },\n            {\n              \"y\": 7,\n              \"x\": 1\n            },\n            {\n              \"y\": 7,\n              \"x\": 2\n            },\n            {\n              \"y\": 7,\n              \"x\": 3\n            },\n            {\n              \"y\": 7,\n              \"x\": 4\n            },\n            {\n              \"y\": 7,\n              \"x\": 5\n            },\n            {\n              \"y\": 7,\n              \"x\": 6\n            },\n            {\n              \"y\": 7,\n              \"x\": 7\n            }\n          ],\n          \"dram\": [\n            {\n              \"y\": 8,\n              \"x\": 0\n            },\n            {\n              \"y\": 9,\n              \"x\": 0\n            },\n            {\n              \"y\": 10,\n              \"x\": 0\n            },\n            {\n              \"y\": 8,\n              \"x\": 1\n            },\n            {\n              \"y\": 9,\n              \"x\": 1\n            },\n            {\n              \"y\": 10,\n              \"x\": 1\n            },\n            {\n              \"y\": 8,\n              \"x\": 2\n            },\n            {\n              \"y\": 9,\n              \"x\": 2\n            },\n            {\n              \"y\": 10,\n              \"x\": 2\n            },\n            {\n              \"y\": 8,\n              \"x\": 3\n            },\n            {\n              \"y\": 9,\n              \"x\": 3\n            },\n            {\n              \"y\": 10,\n              \"x\": 3\n            }\n          ],\n          \"eth\": [\n\n          ],\n          \"eth_inactive\": [\n\n          ]\n        },\n        \"supported_data_types\": [\n          \"Float32\",\n          \"Float16\",\n          \"BFloat16\",\n          \"BFP_Float8\",\n          \"BFP_BFloat8\",\n          \"BFP_Float4\",\n          \"BFP_BFloat4\",\n          \"BFP_Float2\",\n          \"BFP_BFloat2\",\n          \"UInt32\",\n          \"UInt16\",\n          \"UInt8\"\n        ],\n        \"supported_tile_sizes\": [\n          {\n            \"y\": 4,\n            \"x\": 16\n          },\n          {\n            \"y\": 16,\n            \"x\": 16\n          },\n          {\n            \"y\": 32,\n            \"x\": 16\n          },\n          {\n            \"y\": 4,\n            \"x\": 32\n          },\n          {\n            \"y\": 16,\n            \"x\": 32\n          },\n          {\n            \"y\": 32,\n            \"x\": 32\n          }\n        ],\n        \"num_cbs\": 32\n      }\n    ],\n    \"chip_desc_indices\": [\n      0\n    ],\n    \"chip_capabilities\": [\n      3\n    ],\n    \"chip_coords\": [\n      {\n        \"rack\": 0,\n        \"shelf\": 0,\n        \"y\": 0,\n        \"x\": 0\n      }\n    ],\n    \"chip_channels\": [\n\n    ]\n  },\n  \"programs\": [\n    {\n      \"name\": \"main\",\n      \"inputs\": [\n        {\n          \"global_id\": 1,\n          \"address\": 0,\n          \"size\": 0,\n          \"desc\": {\n            \"shape\": [\n              256,\n              256\n            ],\n            \"layout\": {\n              \"stride\": [\n                256,\n                1\n              ],\n              \"oob_val\": \"Undef\",\n              \"core_range_set\": [\n                {\n                  \"loc\": {\n                    \"y\": 0,\n                    \"x\": 0\n                  },\n                  \"size\": {\n                    \"y\": 1,\n                    \"x\": 1\n                  }\n                }\n              ],\n              \"memory_desc\": {\n                \"shape\": [\n                  256,\n                  256\n                ],\n                \"tile_shape\": {\n                  \"y\": 1,\n                  \"x\": 1\n                },\n                \"data_type\": \"Float32\",\n                \"memory_space\": \"System\",\n                \"memory_layout\": \"None\",\n                \"size\": 262144\n              }\n            }\n          }\n        }\n      ],\n      \"outputs\": [\n        {\n          \"global_id\": 10,\n          \"address\": 0,\n          \"size\": 0,\n          \"desc\": {\n            \"shape\": [\n              1\n            ],\n            \"layout\": {\n              \"stride\": [\n                1\n              ],\n              \"oob_val\": \"Undef\",\n              \"core_range_set\": [\n                {\n                  \"loc\": {\n                    \"y\": 0,\n                    \"x\": 0\n                  },\n                  \"size\": {\n                    \"y\": 1,\n                    \"x\": 1\n                  }\n                }\n              ],\n              \"memory_desc\": {\n                \"shape\": [\n                  1,\n                  1\n                ],\n                \"tile_shape\": {\n                  \"y\": 1,\n                  \"x\": 1\n                },\n                \"data_type\": \"Float32\",\n                \"memory_space\": \"System\",\n                \"memory_layout\": \"None\",\n                \"size\": 4\n              }\n            }\n          }\n        }\n      ],\n      \"operations\": [\n        {\n          \"type_type\": \"GetDeviceOp\",\n          \"type\": {\n            \"mesh\": {\n              \"y\": 1,\n              \"x\": 1\n            },\n            \"chip_ids\": [\n              0\n            ],\n            \"out\": {\n              \"global_id\": 0\n            }\n          },\n          \"debug_info\": \"%0 = \\\"ttnn.get_device\\\"() <{mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>> loc(unknown)\",\n          \"loc_info\": \"loc(unknown)\"\n        },\n        {\n          \"type_type\": \"FullOp\",\n          \"type\": {\n            \"device\": {\n              \"global_id\": 0\n            },\n            \"fill_value\": 65536.0,\n            \"num_shards\": 1,\n            \"strategy\": {\n              \"strategy_type\": \"NONE\"\n            },\n            \"out\": {\n              \"global_id\": 2,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"UInt32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 4096\n                  }\n                }\n              }\n            }\n          },\n          \"debug_info\": \"%1 = \\\"ttnn.full\\\"(%0) <{fillValue = 6.553600e+04 : f32}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1xi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)\",\n          \"loc_info\": \"loc(unknown)\"\n        },\n        {\n          \"type_type\": \"ToLayoutOp\",\n          \"type\": {\n            \"in\": {\n              \"global_id\": 1,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  256,\n                  256\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    256,\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      256,\n                      256\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 1,\n                      \"x\": 1\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"System\",\n                    \"memory_layout\": \"None\",\n                    \"size\": 262144\n                  }\n                }\n              }\n            },\n            \"layout\": \"Tile\",\n            \"dtype\": null,\n            \"out\": {\n              \"global_id\": 3,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  256,\n                  256\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    256,\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      8,\n                      8\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 262144\n                  }\n                }\n              }\n            }\n          },\n          \"debug_info\": \"%2 = \\\"ttnn.to_layout\\\"(%arg0) <{layout = #ttnn.layout<tile>}> : (tensor<256x256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x256xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<256x256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x8x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(\\\"-\\\":5:10)\",\n          \"loc_info\": \"loc(\\\"-\\\":5:10)\"\n        },\n        {\n          \"type_type\": \"ToDeviceOp\",\n          \"type\": {\n            \"in\": {\n              \"global_id\": 3,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  256,\n                  256\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    256,\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      8,\n                      8\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 262144\n                  }\n                }\n              }\n            },\n            \"device\": {\n              \"global_id\": 0\n            },\n            \"memcfg\": {\n              \"tensor_memory_layout\": \"Interleaved\",\n              \"buffer_type\": \"DRAM\",\n              \"shard_spec\": {\n                \"shard_shape\": [\n                  8,\n                  8\n                ]\n              }\n            },\n            \"out\": {\n              \"global_id\": 4,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  256,\n                  256\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    256,\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      8,\n                      8\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 262144\n                  }\n                }\n              }\n            }\n          },\n          \"debug_info\": \"%3 = \\\"ttnn.to_device\\\"(%2, %0) <{memory_config = #ttnn.memory_config<<dram>, <<8x8>>, <interleaved>>}> : (tensor<256x256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x8x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<256x256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x8x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(\\\"-\\\":5:10)\",\n          \"loc_info\": \"loc(\\\"-\\\":5:10)\"\n        },\n        {\n          \"type_type\": \"DeallocateOp\",\n          \"type\": {\n            \"in\": {\n              \"global_id\": 3,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  256,\n                  256\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    256,\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      8,\n                      8\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 262144\n                  }\n                }\n              }\n            },\n            \"force\": false\n          },\n          \"debug_info\": \"\\\"ttnn.deallocate\\\"(%2) <{force = false}> : (tensor<256x256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x8x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(\\\"-\\\":5:10)\",\n          \"loc_info\": \"loc(\\\"-\\\":5:10)\"\n        },\n        {\n          \"type_type\": \"ReductionOp\",\n          \"type\": {\n            \"type\": \"Sum\",\n            \"in\": {\n              \"global_id\": 4,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  256,\n                  256\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    256,\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      8,\n                      8\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 262144\n                  }\n                }\n              }\n            },\n            \"out\": {\n              \"global_id\": 5,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1,\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1,\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 4096\n                  }\n                }\n              }\n            },\n            \"keep_dim\": true\n          },\n          \"debug_info\": \"%4 = \\\"ttnn.sum\\\"(%3) <{keep_dim = true}> : (tensor<256x256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x8x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(\\\"-\\\":5:10)\",\n          \"loc_info\": \"loc(\\\"-\\\":5:10)\"\n        },\n        {\n          \"type_type\": \"DeallocateOp\",\n          \"type\": {\n            \"in\": {\n              \"global_id\": 4,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  256,\n                  256\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    256,\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      8,\n                      8\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 262144\n                  }\n                }\n              }\n            },\n            \"force\": false\n          },\n          \"debug_info\": \"\\\"ttnn.deallocate\\\"(%3) <{force = false}> : (tensor<256x256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x8x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(\\\"-\\\":5:10)\",\n          \"loc_info\": \"loc(\\\"-\\\":5:10)\"\n        },\n        {\n          \"type_type\": \"ReshapeOp\",\n          \"type\": {\n            \"in\": {\n              \"global_id\": 5,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1,\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1,\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 4096\n                  }\n                }\n              }\n            },\n            \"out\": {\n              \"global_id\": 6,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 4096\n                  }\n                }\n              }\n            },\n            \"shape\": [\n              1\n            ]\n          },\n          \"debug_info\": \"%5 = \\\"ttnn.reshape\\\"(%4) <{shape = [1 : i32]}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(\\\"-\\\":5:10)\",\n          \"loc_info\": \"loc(\\\"-\\\":5:10)\"\n        },\n        {\n          \"type_type\": \"DeallocateOp\",\n          \"type\": {\n            \"in\": {\n              \"global_id\": 5,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1,\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1,\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 4096\n                  }\n                }\n              }\n            },\n            \"force\": false\n          },\n          \"debug_info\": \"\\\"ttnn.deallocate\\\"(%4) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(\\\"-\\\":5:10)\",\n          \"loc_info\": \"loc(\\\"-\\\":5:10)\"\n        },\n        {\n          \"type_type\": \"TypecastOp\",\n          \"type\": {\n            \"in\": {\n              \"global_id\": 2,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"UInt32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 4096\n                  }\n                }\n              }\n            },\n            \"dtype\": \"Float32\",\n            \"out\": {\n              \"global_id\": 7,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 4096\n                  }\n                }\n              }\n            }\n          },\n          \"debug_info\": \"%6 = \\\"ttnn.typecast\\\"(%1) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1xi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(\\\"-\\\":7:10)\",\n          \"loc_info\": \"loc(\\\"-\\\":7:10)\"\n        },\n        {\n          \"type_type\": \"DeallocateOp\",\n          \"type\": {\n            \"in\": {\n              \"global_id\": 2,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"UInt32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 4096\n                  }\n                }\n              }\n            },\n            \"force\": false\n          },\n          \"debug_info\": \"\\\"ttnn.deallocate\\\"(%1) <{force = false}> : (tensor<1xi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(\\\"-\\\":7:10)\",\n          \"loc_info\": \"loc(\\\"-\\\":7:10)\"\n        },\n        {\n          \"type_type\": \"EmptyOp\",\n          \"type\": {\n            \"shape\": [\n              1\n            ],\n            \"dtype\": \"Float32\",\n            \"layout\": \"Tile\",\n            \"num_shards\": 1,\n            \"device\": {\n              \"global_id\": 0\n            },\n            \"memcfg\": {\n              \"tensor_memory_layout\": \"Interleaved\",\n              \"buffer_type\": \"DRAM\",\n              \"shard_spec\": {\n                \"shard_shape\": [\n                  1,\n                  1\n                ]\n              }\n            },\n            \"strategy\": {\n              \"strategy_type\": \"NONE\"\n            },\n            \"out\": {\n              \"global_id\": 8,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 4096\n                  }\n                }\n              }\n            }\n          },\n          \"debug_info\": \"%7 = \\\"ttnn.empty\\\"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(\\\"-\\\":10:10)\",\n          \"loc_info\": \"loc(\\\"-\\\":10:10)\"\n        },\n        {\n          \"type_type\": \"EltwiseOp\",\n          \"type\": {\n            \"type\": \"Div\",\n            \"ins\": [\n              {\n                \"global_id\": 6,\n                \"address\": 0,\n                \"size\": 0,\n                \"desc\": {\n                  \"shape\": [\n                    1\n                  ],\n                  \"layout\": {\n                    \"stride\": [\n                      1\n                    ],\n                    \"oob_val\": \"Undef\",\n                    \"core_range_set\": [\n                      {\n                        \"loc\": {\n                          \"y\": 0,\n                          \"x\": 0\n                        },\n                        \"size\": {\n                          \"y\": 1,\n                          \"x\": 1\n                        }\n                      }\n                    ],\n                    \"memory_desc\": {\n                      \"shape\": [\n                        1,\n                        1\n                      ],\n                      \"tile_shape\": {\n                        \"y\": 32,\n                        \"x\": 32\n                      },\n                      \"data_type\": \"Float32\",\n                      \"memory_space\": \"DeviceDRAM\",\n                      \"memory_layout\": \"Interleaved\",\n                      \"size\": 4096\n                    }\n                  }\n                }\n              },\n              {\n                \"global_id\": 7,\n                \"address\": 0,\n                \"size\": 0,\n                \"desc\": {\n                  \"shape\": [\n                    1\n                  ],\n                  \"layout\": {\n                    \"stride\": [\n                      1\n                    ],\n                    \"oob_val\": \"Undef\",\n                    \"core_range_set\": [\n                      {\n                        \"loc\": {\n                          \"y\": 0,\n                          \"x\": 0\n                        },\n                        \"size\": {\n                          \"y\": 1,\n                          \"x\": 1\n                        }\n                      }\n                    ],\n                    \"memory_desc\": {\n                      \"shape\": [\n                        1,\n                        1\n                      ],\n                      \"tile_shape\": {\n                        \"y\": 32,\n                        \"x\": 32\n                      },\n                      \"data_type\": \"Float32\",\n                      \"memory_space\": \"DeviceDRAM\",\n                      \"memory_layout\": \"Interleaved\",\n                      \"size\": 4096\n                    }\n                  }\n                }\n              }\n            ],\n            \"out\": {\n              \"global_id\": 8,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 4096\n                  }\n                }\n              }\n            },\n            \"params_type\": \"NONE\"\n          },\n          \"debug_info\": \"%8 = \\\"ttnn.div\\\"(%5, %6, %7) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(\\\"-\\\":11:10)\",\n          \"loc_info\": \"loc(\\\"-\\\":11:10)\"\n        },\n        {\n          \"type_type\": \"DeallocateOp\",\n          \"type\": {\n            \"in\": {\n              \"global_id\": 7,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 4096\n                  }\n                }\n              }\n            },\n            \"force\": false\n          },\n          \"debug_info\": \"\\\"ttnn.deallocate\\\"(%6) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(\\\"-\\\":11:10)\",\n          \"loc_info\": \"loc(\\\"-\\\":11:10)\"\n        },\n        {\n          \"type_type\": \"DeallocateOp\",\n          \"type\": {\n            \"in\": {\n              \"global_id\": 6,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 4096\n                  }\n                }\n              }\n            },\n            \"force\": false\n          },\n          \"debug_info\": \"\\\"ttnn.deallocate\\\"(%5) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(\\\"-\\\":11:10)\",\n          \"loc_info\": \"loc(\\\"-\\\":11:10)\"\n        },\n        {\n          \"type_type\": \"FromDeviceOp\",\n          \"type\": {\n            \"in\": {\n              \"global_id\": 8,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 4096\n                  }\n                }\n              }\n            },\n            \"out\": {\n              \"global_id\": 9,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 1,\n                      \"x\": 1\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"System\",\n                    \"memory_layout\": \"None\",\n                    \"size\": 4\n                  }\n                }\n              }\n            }\n          },\n          \"debug_info\": \"%9 = \\\"ttnn.from_device\\\"(%8) : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xf32, #ttnn.buffer_type<system_memory>>>> loc(\\\"-\\\":12:5)\",\n          \"loc_info\": \"loc(\\\"-\\\":12:5)\"\n        },\n        {\n          \"type_type\": \"DeallocateOp\",\n          \"type\": {\n            \"in\": {\n              \"global_id\": 8,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 32,\n                      \"x\": 32\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"DeviceDRAM\",\n                    \"memory_layout\": \"Interleaved\",\n                    \"size\": 4096\n                  }\n                }\n              }\n            },\n            \"force\": false\n          },\n          \"debug_info\": \"\\\"ttnn.deallocate\\\"(%7) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(\\\"-\\\":12:5)\",\n          \"loc_info\": \"loc(\\\"-\\\":12:5)\"\n        },\n        {\n          \"type_type\": \"ToLayoutOp\",\n          \"type\": {\n            \"in\": {\n              \"global_id\": 9,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 1,\n                      \"x\": 1\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"System\",\n                    \"memory_layout\": \"None\",\n                    \"size\": 4\n                  }\n                }\n              }\n            },\n            \"layout\": \"RowMajor\",\n            \"dtype\": null,\n            \"out\": {\n              \"global_id\": 10,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 1,\n                      \"x\": 1\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"System\",\n                    \"memory_layout\": \"None\",\n                    \"size\": 4\n                  }\n                }\n              }\n            }\n          },\n          \"debug_info\": \"%10 = \\\"ttnn.to_layout\\\"(%9) <{layout = #ttnn.layout<row_major>}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xf32, #ttnn.buffer_type<system_memory>>>> loc(\\\"-\\\":12:5)\",\n          \"loc_info\": \"loc(\\\"-\\\":12:5)\"\n        },\n        {\n          \"type_type\": \"DeallocateOp\",\n          \"type\": {\n            \"in\": {\n              \"global_id\": 9,\n              \"address\": 0,\n              \"size\": 0,\n              \"desc\": {\n                \"shape\": [\n                  1\n                ],\n                \"layout\": {\n                  \"stride\": [\n                    1\n                  ],\n                  \"oob_val\": \"Undef\",\n                  \"core_range_set\": [\n                    {\n                      \"loc\": {\n                        \"y\": 0,\n                        \"x\": 0\n                      },\n                      \"size\": {\n                        \"y\": 1,\n                        \"x\": 1\n                      }\n                    }\n                  ],\n                  \"memory_desc\": {\n                    \"shape\": [\n                      1,\n                      1\n                    ],\n                    \"tile_shape\": {\n                      \"y\": 1,\n                      \"x\": 1\n                    },\n                    \"data_type\": \"Float32\",\n                    \"memory_space\": \"System\",\n                    \"memory_layout\": \"None\",\n                    \"size\": 4\n                  }\n                }\n              }\n            },\n            \"force\": false\n          },\n          \"debug_info\": \"\\\"ttnn.deallocate\\\"(%9) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xf32, #ttnn.buffer_type<system_memory>>>>) -> () loc(\\\"-\\\":12:5)\",\n          \"loc_info\": \"loc(\\\"-\\\":12:5)\"\n        }\n      ],\n      \"debug_info\": {\n        \"mlir\": {\n          \"name\": \"ttnn\",\n          \"source\": \"#device = #tt.device<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>\\n#dram = #ttnn.buffer_type<dram>\\n#loc2 = loc(\\\"-\\\":2:19)\\n#system_desc = #tt.system_desc<[{role = host, target_triple = \\\"x86_64-pc-linux-gnu\\\"}], [{arch = <wormhole_b0>, grid = 8x8, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 1024, erisc_l1_unreserved_base = 1024, dram_unreserved_base = 1024, dram_unreserved_end = 1073741824, physical_cores = {worker = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  1x0,  1x1,  1x2,  1x3,  1x4,  1x5,  1x6,  1x7,  2x0,  2x1,  2x2,  2x3,  2x4,  2x5,  2x6,  2x7,  3x0,  3x1,  3x2,  3x3,  3x4,  3x5,  3x6,  3x7,  4x0,  4x1,  4x2,  4x3,  4x4,  4x5,  4x6,  4x7,  5x0,  5x1,  5x2,  5x3,  5x4,  5x5,  5x6,  5x7,  6x0,  6x1,  6x2,  6x3,  6x4,  6x5,  6x6,  6x7,  7x0,  7x1,  7x2,  7x3,  7x4,  7x5,  7x6,  7x7] dram = [ 8x0,  9x0,  10x0,  8x1,  9x1,  10x1,  8x2,  9x2,  10x2,  8x3,  9x3,  10x3]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32}], [0], [3 : i32], [ 0x0x0x0]>\\n#system_memory = #ttnn.buffer_type<system_memory>\\n#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x256xf32, #system_memory>>\\n#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xf32, #system_memory>>\\n#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>\\n#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x8x!tt.tile<32x32, f32>, #dram>, <interleaved>>\\n#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>\\n#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>\\nmodule attributes {tt.device = #device, tt.system_desc = #system_desc} {\\n  func.func @main(%arg0: tensor<256x256xf32, #ttnn_layout> loc(\\\"-\\\":2:19)) -> tensor<1xf32, #ttnn_layout1> {\\n    %0 = \\\"ttnn.get_device\\\"() <{mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !tt.device<#device> loc(#loc3)\\n    %1 = \\\"ttnn.full\\\"(%0) <{fillValue = 6.553600e+04 : f32}> : (!tt.device<#device>) -> tensor<1xi32, #ttnn_layout2> loc(#loc3)\\n    %2 = \\\"ttnn.to_layout\\\"(%arg0) <{layout = #ttnn.layout<tile>}> : (tensor<256x256xf32, #ttnn_layout>) -> tensor<256x256xf32, #ttnn_layout3> loc(#loc4)\\n    %3 = \\\"ttnn.to_device\\\"(%2, %0) <{memory_config = #ttnn.memory_config<#dram, <<8x8>>, <interleaved>>}> : (tensor<256x256xf32, #ttnn_layout3>, !tt.device<#device>) -> tensor<256x256xf32, #ttnn_layout3> loc(#loc4)\\n    \\\"ttnn.deallocate\\\"(%2) <{force = false}> : (tensor<256x256xf32, #ttnn_layout3>) -> () loc(#loc4)\\n    %4 = \\\"ttnn.sum\\\"(%3) <{keep_dim = true}> : (tensor<256x256xf32, #ttnn_layout3>) -> tensor<1x1xf32, #ttnn_layout4> loc(#loc4)\\n    \\\"ttnn.deallocate\\\"(%3) <{force = false}> : (tensor<256x256xf32, #ttnn_layout3>) -> () loc(#loc4)\\n    %5 = \\\"ttnn.reshape\\\"(%4) <{shape = [1 : i32]}> : (tensor<1x1xf32, #ttnn_layout4>) -> tensor<1xf32, #ttnn_layout5> loc(#loc4)\\n    \\\"ttnn.deallocate\\\"(%4) <{force = false}> : (tensor<1x1xf32, #ttnn_layout4>) -> () loc(#loc4)\\n    %6 = \\\"ttnn.typecast\\\"(%1) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1xi32, #ttnn_layout2>) -> tensor<1xf32, #ttnn_layout5> loc(#loc5)\\n    \\\"ttnn.deallocate\\\"(%1) <{force = false}> : (tensor<1xi32, #ttnn_layout2>) -> () loc(#loc5)\\n    %7 = \\\"ttnn.empty\\\"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1>}> : (!tt.device<#device>) -> tensor<1xf32, #ttnn_layout5> loc(#loc6)\\n    %8 = \\\"ttnn.div\\\"(%5, %6, %7) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1xf32, #ttnn_layout5>, tensor<1xf32, #ttnn_layout5>, tensor<1xf32, #ttnn_layout5>) -> tensor<1xf32, #ttnn_layout5> loc(#loc7)\\n    \\\"ttnn.deallocate\\\"(%6) <{force = false}> : (tensor<1xf32, #ttnn_layout5>) -> () loc(#loc7)\\n    \\\"ttnn.deallocate\\\"(%5) <{force = false}> : (tensor<1xf32, #ttnn_layout5>) -> () loc(#loc7)\\n    %9 = \\\"ttnn.from_device\\\"(%8) : (tensor<1xf32, #ttnn_layout5>) -> tensor<1xf32, #ttnn_layout1> loc(#loc8)\\n    \\\"ttnn.deallocate\\\"(%7) <{force = false}> : (tensor<1xf32, #ttnn_layout5>) -> () loc(#loc8)\\n    %10 = \\\"ttnn.to_layout\\\"(%9) <{layout = #ttnn.layout<row_major>}> : (tensor<1xf32, #ttnn_layout1>) -> tensor<1xf32, #ttnn_layout1> loc(#loc8)\\n    \\\"ttnn.deallocate\\\"(%9) <{force = false}> : (tensor<1xf32, #ttnn_layout1>) -> () loc(#loc8)\\n    return %10 : tensor<1xf32, #ttnn_layout1> loc(#loc8)\\n  } loc(#loc1)\\n} loc(#loc)\\n#loc = loc(\\\"-\\\":1:1)\\n#loc1 = loc(\\\"-\\\":2:3)\\n#loc3 = loc(unknown)\\n#loc4 = loc(\\\"-\\\":5:10)\\n#loc5 = loc(\\\"-\\\":7:10)\\n#loc6 = loc(\\\"-\\\":10:10)\\n#loc7 = loc(\\\"-\\\":11:10)\\n#loc8 = loc(\\\"-\\\":12:5)\\n\"\n        },\n        \"cpp\": \"#include \\\"ttnn-precompiled.hpp\\\"\\nttnn::Tensor main(ttnn::Tensor v1) {\\n  ttnn::IDevice* v2 = ttnn::DeviceGetter::getInstance();\\n  ttnn::Tensor v3 = ttnn::full(v2);\\n  ttnn::Tensor v4 = ttnn::to_layout(v1, ttnn::Layout::TILE, std::nullopt, std::nullopt, static_cast<::ttnn::IDevice *>(nullptr));\\n  ttnn::MemoryConfig v5 = ttnn::MemoryConfig(ttnn::TensorMemoryLayout::INTERLEAVED, ttnn::BufferType::DRAM);\\n  ttnn::Tensor v6 = ttnn::to_device(v4, v2, v5);\\n  ttnn::deallocate(v4, false);\\n  ttnn::Tensor v7 = ttnn::sum(v6);\\n  ttnn::deallocate(v6, false);\\n  ttnn::Tensor v8 = ttnn::reshape(v7);\\n  ttnn::deallocate(v7, false);\\n  ttnn::Tensor v9 = ttnn::typecast(v3);\\n  ttnn::deallocate(v3, false);\\n  ttnn::Shape v10 = ttnn::Shape(tt::tt_metal::LegacyShape({1, }));\\n  ttnn::MemoryConfig v11 = ttnn::MemoryConfig(ttnn::TensorMemoryLayout::INTERLEAVED, ttnn::BufferType::DRAM);\\n  ttnn::Tensor v12 = ttnn::empty(v10, ttnn::DataType::FLOAT32, ttnn::Layout::TILE, v2, v11);\\n  ttnn::Tensor v13 = ttnn::div(v8, v9, v12);\\n  ttnn::deallocate(v9, false);\\n  ttnn::deallocate(v8, false);\\n  ttnn::Tensor v14 = ttnn::from_device(v13);\\n  ttnn::deallocate(v12, false);\\n  ttnn::Tensor v15 = ttnn::to_layout(v14, ttnn::Layout::ROW_MAJOR, std::nullopt, std::nullopt, static_cast<::ttnn::IDevice *>(nullptr));\\n  ttnn::deallocate(v14, false);\\n  return v15;\\n}\\n\\n\\n\",\n        \"golden_info\": {\n          \"golden_map\": [\n\n          ]\n        }\n      }\n    }\n  ]\n}\n"}}
